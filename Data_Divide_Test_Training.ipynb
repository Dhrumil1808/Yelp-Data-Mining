{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading data from yelp data set sql dump. We have considered data using following constraints:\n",
    "- Users who have written 50 or more reviews\n",
    "- Their reviews\n",
    "- Rating of each of the review\n",
    "\n",
    "This data is grouped into 80% Training data and 20% Test data.\n",
    "We have grouped data by user ID and then split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split(review_data, testperc):\n",
    "    uids=review_data['UID'].unique()\n",
    "    full_size = len(uids)\n",
    "    choose = (int)(((float)(testperc)/100) * full_size)\n",
    "    indices = np.random.choice(len(uids), choose, replace=False)\n",
    "    full_indices = [m for m in range(0,len(uids))]\n",
    "    train_indices = set(full_indices)-set(indices)\n",
    "    grouped =review_data.groupby([\"UID\"])\n",
    "    saveGroupedData(\"data/test_temp.dat\",indices,grouped ,uids)\n",
    "    saveGroupedData(\"data/train_temp.dat\",train_indices,grouped,uids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveGroupedData(filename, indices, grouped,uids):\n",
    "\n",
    "    output_file = open(filename, 'w')\n",
    "    for i in indices:\n",
    "        indexr = uids[i]\n",
    "        grouped_data = grouped.get_group(indexr)\n",
    "        size = len(grouped_data.Stars)\n",
    "        ctr=0\n",
    "        stars = list(grouped_data.Stars)\n",
    "        reviews = list(grouped_data.Review)\n",
    "        while(ctr <size):\n",
    "            #str1 = str(indexr) +\"\\t\"\n",
    "            str1 =str(reviews[ctr])\n",
    "            #str1 =str1 + str(stars[ctr])\n",
    "            output_file.write(str1+\"\\n\")\n",
    "            ctr=ctr+1\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data_all = pd.read_csv('data/reviews_with_counter.dat',\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking only 100K data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data = review_data_all[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_train_split (review_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/reviews.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/test_temp.dat\", \"r\") as fh:\n",
    "    test_idx = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_temp.dat\", \"r\") as fh:\n",
    "    train_idx = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open(\"data/train_final.dat\", 'w')\n",
    "for i in train_idx:\n",
    "    train_file.write(lines[int(i)-1])\n",
    "train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = open(\"data/test_final.dat\", 'w')\n",
    "for i in test_idx:\n",
    "    test_file.write(lines[int(i)-1])\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
