{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading data from yelp data set sql dump. We have considered data using following constraints:\n",
    "- Users who have written 50 or more reviews\n",
    "- Their reviews\n",
    "- Rating of each of the review\n",
    "\n",
    "This data is grouped into 80% Training data and 20% Test data.\n",
    "We have grouped data by user ID and then split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split_per_groupby(review_data,groupby, testperc):\n",
    "    uids=review_data[groupby].unique()\n",
    "    full_size = len(uids)\n",
    "    choose = (int)(((float)(testperc)/100) * full_size)\n",
    "    indices = np.random.choice(len(uids), choose, replace=False)\n",
    "    full_indices = [m for m in range(0,len(uids))]\n",
    "    train_indices = set(full_indices)-set(indices)\n",
    "    grouped =review_data.groupby([groupby])\n",
    "    test_indices_final=[]  \n",
    "    train_indices_final = []\n",
    "    for g in grouped.groups:\n",
    "        full_size = len(grouped.groups[g])\n",
    "        choose = (int)(((float)(testperc)/100) * full_size)\n",
    "        indices = np.random.choice(full_size, choose, replace=False)\n",
    "        full_indices = [m for m in range(0,full_size)]\n",
    "        train_indices = set(full_indices)-set(indices)\n",
    "        train_indices = list(train_indices)\n",
    "        test_indices_final.extend(list(grouped.groups[g][indices]))\n",
    "        train_indices_final.extend(list(grouped.groups[g][train_indices]))\n",
    "    saveGroupedData(\"data/test_\"+str(groupby)+\".dat\",test_indices_final)\n",
    "    saveGroupedData(\"data/train_\"+str(groupby)+\".dat\",train_indices_final)\n",
    "\n",
    "def saveGroupedData(filename, indices):\n",
    "    output_file = open(filename, 'w')\n",
    "    for i in indices:\n",
    "        output_file.write(str(i)+\"\\n\")\n",
    "    output_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data_all = pd.read_csv('data/reviews_no_text.dat',\"\\t\")\n",
    "train_indices_user = test_train_split_per_groupby(review_data_all,'UID',20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pickle files for user local model. i.e all per user reviews split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "user_id = ''\n",
    "filename = 'pickle/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "\n",
    "with open(\"data/test_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data for Business Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pickle/train/uid/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "\n",
    "\n",
    "datframeb={}\n",
    "datframeb['BusinessId']=business\n",
    "datframeb = pd.DataFrame(datframeb)\n",
    "test_train_split_per_groupby(datframeb,'BusinessId',20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/test_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
