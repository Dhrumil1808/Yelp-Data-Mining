{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading data from yelp data set sql dump. We have considered data using following constraints:\n",
    "- Users who have written 50 or more reviews\n",
    "- Their reviews\n",
    "- Rating of each of the review\n",
    "\n",
    "This data is grouped into 80% Training data and 20% Test data.\n",
    "We have grouped data by user ID and then split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split_per_groupby(review_data,groupby, testperc):\n",
    "    uids=review_data[groupby].unique()\n",
    "    full_size = len(uids)\n",
    "    choose = (int)(((float)(testperc)/100) * full_size)\n",
    "    indices = np.random.choice(len(uids), choose, replace=False)\n",
    "    full_indices = [m for m in range(0,len(uids))]\n",
    "    train_indices = set(full_indices)-set(indices)\n",
    "    grouped =review_data.groupby([groupby])\n",
    "    test_indices_final=[]  \n",
    "    train_indices_final = []\n",
    "    for g in grouped.groups:\n",
    "        full_size = len(grouped.groups[g])\n",
    "        choose = (int)(((float)(testperc)/100) * full_size)\n",
    "        indices = np.random.choice(full_size, choose, replace=False)\n",
    "        full_indices = [m for m in range(0,full_size)]\n",
    "        train_indices = set(full_indices)-set(indices)\n",
    "        train_indices = list(train_indices)\n",
    "        test_indices_final.extend(list(grouped.groups[g][indices]))\n",
    "        train_indices_final.extend(list(grouped.groups[g][train_indices]))\n",
    "    saveGroupedData(\"data/test_\"+str(groupby)+\".dat\",test_indices_final)\n",
    "    saveGroupedData(\"data/train_\"+str(groupby)+\".dat\",train_indices_final)\n",
    "\n",
    "def saveGroupedData(filename, indices):\n",
    "    output_file = open(filename, 'w')\n",
    "    for i in indices:\n",
    "        output_file.write(str(i)+\"\\n\")\n",
    "    output_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data_all = pd.read_csv('data/reviews_no_text.dat',\"\\t\")\n",
    "num_samples=50000\n",
    "review_data_all=review_data_all[:num_samples]\n",
    "train_indices_user = test_train_split_per_groupby(review_data_all,'UID',20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pickle files for user local model. i.e all per user reviews split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "user_id = ''\n",
    "filename = 'pickle/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "docs_nmf = load_pickle(filename+'nmf.pickle')\n",
    "\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "doc_full = []\n",
    "with open(\"data/test_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')\n",
    "save_pickle(docs_nmf[lines1], filename+'docs_nmf.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "doc_full = []\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')\n",
    "save_pickle(docs_nmf[lines1], filename+'docs_nmf.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data for Business Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pickle/train/uid/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "\n",
    "\n",
    "datframeb={}\n",
    "datframeb['BusinessId']=business\n",
    "datframeb = pd.DataFrame(datframeb)\n",
    "test_train_split_per_groupby(datframeb,'BusinessId',20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Number of lines processed: 317586\n",
      "Training Data: Length of userid array: 317586\n",
      "Training Data: Length of rating array: 317586\n",
      "Training Data: Length of docs array: 317586\n",
      "Training Data: Length of business array: 317586\n",
      "Training Data: Number of exceptions encountered: 0\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 9.823s.\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=50000 and n_features=15000...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f48e6ea912f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m           l1_ratio=.5).fit(tfidf)\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtfidf_nmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                                                   \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg_H\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                                   \u001b[0ml2_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                                                   verbose)\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_fit_multiplicative_update\u001b[0;34m(X, W, H, beta_loss, max_iter, tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose)\u001b[0m\n\u001b[1;32m    775\u001b[0m         delta_W, H_sum, HHt, XHt = _multiplicative_update_w(\n\u001b[1;32m    776\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             H_sum, HHt, XHt, update_H)\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdelta_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_multiplicative_update_w\u001b[0;34m(X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma, H_sum, HHt, XHt, update_H)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# Numerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# if X is sparse, compute WH only where X is non zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mWH_safe_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_special_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mWH_safe_X_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWH_safe_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc\u001b[0m in \u001b[0;36m_special_sparse_dot\u001b[0;34m(W, H, X)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mdot_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mWH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mWH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rashmisharma/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lines=[]\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()  \n",
    "    \n",
    "userid = []\n",
    "rating = []\n",
    "docs = []\n",
    "business = []\n",
    "i = 0\n",
    "j = 0\n",
    "error_line_num = []\n",
    "error_lines = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        i = i + 1\n",
    "        l = line.split('\\t', 4)\n",
    "        userid.append(l[0])\n",
    "        business.append(l[1])\n",
    "        rating.append(l[2])\n",
    "        docs.append(l[3])\n",
    "        #d = clean(l[3])\n",
    "        #kmers = getKmers(d)\n",
    "        #d.extend(kmers)\n",
    "        #docs.append(d)\n",
    "    except Exception as e:\n",
    "        j = j + 1\n",
    "        error_line_num.append(i)\n",
    "        error_lines.append(line)\n",
    "\n",
    "print 'Training Data: Number of lines processed: ' + str(i)\n",
    "print 'Training Data: Length of userid array: ' + str(len(userid))\n",
    "print 'Training Data: Length of rating array: ' + str(len(rating))\n",
    "print 'Training Data: Length of docs array: ' + str(len(docs))\n",
    "print 'Training Data: Length of business array: ' + str(len(business))\n",
    "print 'Training Data: Number of exceptions encountered: ' + str(j)\n",
    "\n",
    "\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_samples = 50000\n",
    "n_features = 15000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "data_samples = docs[:n_samples]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_nmf = nmf.transform(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "\n",
    "vocab = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "save_pickle(tfidf_nmf, filename+'nmf.pickle')\n",
    "save_pickle(vocab, filename+'vocab.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/test_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "doc_full = []\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "doc_full = []\n",
    "\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
