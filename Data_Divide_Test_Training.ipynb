{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading data from yelp data set sql dump. We have considered data using following constraints:\n",
    "- Users who have written 50 or more reviews\n",
    "- Their reviews\n",
    "- Rating of each of the review\n",
    "\n",
    "This data is grouped into 80% Training data and 20% Test data.\n",
    "We have grouped data by user ID and then split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split_per_groupby(review_data,groupby, testperc):\n",
    "    uids=review_data[groupby].unique()\n",
    "    full_size = len(uids)\n",
    "    choose = (int)(((float)(testperc)/100) * full_size)\n",
    "    indices = np.random.choice(len(uids), choose, replace=False)\n",
    "    full_indices = [m for m in range(0,len(uids))]\n",
    "    train_indices = set(full_indices)-set(indices)\n",
    "    grouped =review_data.groupby([groupby])\n",
    "    test_indices_final=[]  \n",
    "    train_indices_final = []\n",
    "    for g in grouped.groups:\n",
    "        full_size = len(grouped.groups[g])\n",
    "        choose = (int)(((float)(testperc)/100) * full_size)\n",
    "        indices = np.random.choice(full_size, choose, replace=False)\n",
    "        full_indices = [m for m in range(0,full_size)]\n",
    "        train_indices = set(full_indices)-set(indices)\n",
    "        train_indices = list(train_indices)\n",
    "        test_indices_final.extend(list(grouped.groups[g][indices]))\n",
    "        train_indices_final.extend(list(grouped.groups[g][train_indices]))\n",
    "    saveGroupedData(\"data/test_\"+str(groupby)+\".dat\",test_indices_final)\n",
    "    saveGroupedData(\"data/train_\"+str(groupby)+\".dat\",train_indices_final)\n",
    "\n",
    "def saveGroupedData(filename, indices):\n",
    "    output_file = open(filename, 'w')\n",
    "    for i in indices:\n",
    "        output_file.write(str(i)+\"\\n\")\n",
    "    output_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data_all = pd.read_csv('data/reviews_no_text.dat',\"\\t\")\n",
    "num_samples=50000\n",
    "review_data_all=review_data_all[:num_samples]\n",
    "train_indices_user = test_train_split_per_groupby(review_data_all,'UID',20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pickle files for user local model. i.e all per user reviews split into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "user_id = ''\n",
    "filename = 'pickle/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "docs_nmf = load_pickle(filename+'nmf.pickle')\n",
    "\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "doc_full = []\n",
    "with open(\"data/test_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')\n",
    "save_pickle(docs_nmf[lines1], filename+'docs_nmf.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_UID.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "doc_full = []\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/uid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')\n",
    "save_pickle(docs_nmf[lines1], filename+'docs_nmf.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data for Business Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pickle/train/uid/'\n",
    "user = load_pickle(filename+'userid.pickle')\n",
    "ratings = load_pickle(filename+'rating.pickle')\n",
    "business = load_pickle(filename+'business.pickle')\n",
    "docs_csr = load_pickle(filename+'docs_csr.pickle')\n",
    "unique_users = set(user)\n",
    "unique_business = set(business)\n",
    "\n",
    "\n",
    "datframeb={}\n",
    "datframeb['BusinessId']=business\n",
    "datframeb = pd.DataFrame(datframeb)\n",
    "test_train_split_per_groupby(datframeb,'BusinessId',20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Number of lines processed: 317586\n",
      "Training Data: Length of userid array: 317586\n",
      "Training Data: Length of rating array: 317586\n",
      "Training Data: Length of docs array: 317586\n",
      "Training Data: Length of business array: 317586\n",
      "Training Data: Number of exceptions encountered: 0\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.275s.\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=1000 and n_features=15000...\n",
      "done in 3.203s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lines=[]\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()  \n",
    "    \n",
    "userid = []\n",
    "rating = []\n",
    "docs = []\n",
    "business = []\n",
    "i = 0\n",
    "j = 0\n",
    "error_line_num = []\n",
    "error_lines = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        i = i + 1\n",
    "        l = line.split('\\t', 4)\n",
    "        userid.append(l[0])\n",
    "        business.append(l[1])\n",
    "        rating.append(l[2])\n",
    "        docs.append(l[3])\n",
    "        #d = clean(l[3])\n",
    "        #kmers = getKmers(d)\n",
    "        #d.extend(kmers)\n",
    "        #docs.append(d)\n",
    "    except Exception as e:\n",
    "        j = j + 1\n",
    "        error_line_num.append(i)\n",
    "        error_lines.append(line)\n",
    "\n",
    "print 'Training Data: Number of lines processed: ' + str(i)\n",
    "print 'Training Data: Length of userid array: ' + str(len(userid))\n",
    "print 'Training Data: Length of rating array: ' + str(len(rating))\n",
    "print 'Training Data: Length of docs array: ' + str(len(docs))\n",
    "print 'Training Data: Length of business array: ' + str(len(business))\n",
    "print 'Training Data: Number of exceptions encountered: ' + str(j)\n",
    "\n",
    "\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_samples = 50000\n",
    "n_features = 15000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "data_samples = docs[:n_samples]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_nmf = nmf.transform(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "\n",
    "vocab = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "save_pickle(tfidf_nmf, filename+'nmf.pickle')\n",
    "save_pickle(vocab, filename+'vocab.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/test_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "doc_full = []\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/test/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_BusinessId.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    original_docs_lines = fh.readlines()  \n",
    "\n",
    "doc_full = []\n",
    "\n",
    "\n",
    "user2 = []\n",
    "ratings2 = []\n",
    "business2 = []\n",
    "#docs_csr2 = []\n",
    "for i in lines:\n",
    "    user2.append(user[int(i)])\n",
    "    ratings2.append(ratings[int(i)])\n",
    "    business2.append(business[int(i)])\n",
    "    doc_full.append(original_docs_lines[int(i)].split('\\t', 4)[3])\n",
    "\n",
    "\n",
    "lines1 = map(int, lines)\n",
    "filename = 'pickle/train/businessid/'\n",
    "save_pickle(user2, filename+'userid.pickle')\n",
    "save_pickle(ratings2, filename+'rating.pickle')\n",
    "save_pickle(business2, filename+'business.pickle')\n",
    "save_pickle(docs_csr[lines1], filename+'docs_csr.pickle')\n",
    "save_pickle(doc_full, filename+'docs_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
