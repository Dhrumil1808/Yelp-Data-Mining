{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading Test and training data set we created from yelp data set.\n",
    "This data is cleansed and converted to CSR Matrix. This CSR Matrix is huge hence storing it as pickle for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting full Data to CSR Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n-Hit0Y3O6FxTiEhdEEsZw\\t2fqHO2vFbcwVV27c_Irm8Q\\t1\\tCame on Valentine's Day night having pre-bought tickets. We ended up waiting in line for over ten minutes (missing the beginning of our movie) because all of their automated ticketing machines were broken and they weren't properly staffed at the ticket counters. Just to pick up tickets that we had already bought!!! Seriously annoying from a theater that usually is so enjoyable.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Number of lines processed: 317586\n",
      "Training Data: Length of userid array: 317586\n",
      "Training Data: Length of rating array: 317586\n",
      "Training Data: Length of docs array: 317586\n",
      "Training Data: Length of business array: 317586\n",
      "Training Data: Number of exceptions encountered: 0\n"
     ]
    }
   ],
   "source": [
    "userid = []\n",
    "rating = []\n",
    "docs = []\n",
    "business = []\n",
    "i = 0\n",
    "j = 0\n",
    "error_line_num = []\n",
    "error_lines = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        i = i + 1\n",
    "        l = line.split('\\t', 4)\n",
    "        userid.append(l[0])\n",
    "        business.append(l[1])\n",
    "        rating.append(l[2])\n",
    "        d = clean(l[3]).split()\n",
    "        kmers = getKmers(d)\n",
    "        d.extend(kmers)\n",
    "        docs.append(d)\n",
    "    except Exception as e:\n",
    "        j = j + 1\n",
    "        error_line_num.append(i)\n",
    "        error_lines.append(line)\n",
    "        print e\n",
    "\n",
    "print 'Training Data: Number of lines processed: ' + str(i)\n",
    "print 'Training Data: Length of userid array: ' + str(len(userid))\n",
    "print 'Training Data: Length of rating array: ' + str(len(rating))\n",
    "print 'Training Data: Length of docs array: ' + str(len(docs))\n",
    "print 'Training Data: Length of business array: ' + str(len(business))\n",
    "print 'Training Data: Number of exceptions encountered: ' + str(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSR Matrix for complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csr_mat = build_matrix(docs)\n",
    "print 'Training CSR Formed'\n",
    "mat1 = csr_idf(csr_mat, copy=True)\n",
    "print 'Training IDF Formed'\n",
    "docs_csr = csr_l2normalize(mat1, copy=True)\n",
    "print 'Training L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pickle/'\n",
    "save_pickle(userid, filename+'userid.pickle')\n",
    "save_pickle(rating, filename+'rating.pickle')\n",
    "save_pickle(business, filename+'business.pickle')\n",
    "save_pickle(docs_csr, filename+'docs_csr.pickle')\n",
    "print 'Training Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Code to be ignored as it is for previous version of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"data/test_final.dat\", \"r\") as fh:\n",
    "#     test_lines = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_userid = []\n",
    "# test_rating = []\n",
    "# test_docs = []\n",
    "# test_i = 0\n",
    "# test_j = 0\n",
    "# test_error_line_num = []\n",
    "# test_error_lines = []\n",
    "# for line in test_lines:\n",
    "#     try:\n",
    "#         test_i = test_i + 1\n",
    "#         l = line.split('\\t', 3)\n",
    "#         test_userid.append(l[0])\n",
    "#         test_rating.append(l[1])\n",
    "#         d = clean(l[2]).split()\n",
    "#         kmers = getKmers(d)\n",
    "#         d.extend(kmers)\n",
    "#         test_docs.append(d)\n",
    "#     except Exception as e:\n",
    "#         test_j = test_j + 1\n",
    "#         test_error_line_num.append(test_i)\n",
    "#         test_error_lines.append(line)\n",
    "#         print e\n",
    "\n",
    "# print 'Testing Data: Number of lines processed: ' + str(test_i)\n",
    "# print 'Testing Data: Length of userid array: ' + str(len(test_userid))\n",
    "# print 'Testing Data: Length of rating array: ' + str(len(test_rating))\n",
    "# print 'Testing Data: Length of docs array: ' + str(len(test_docs))\n",
    "# print 'Testing Data: Number of exceptions encountered: ' + str(test_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For Training data\n",
    "# csr_mat = build_matrix(docs)\n",
    "# print 'Training CSR Formed'\n",
    "# mat1 = csr_idf(csr_mat, copy=True)\n",
    "# print 'Training IDF Formed'\n",
    "# docs_csr = csr_l2normalize(mat1, copy=True)\n",
    "# print 'Training L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/training/'\n",
    "# save_pickle(userid, filename+'userid.pickle')\n",
    "# save_pickle(rating, filename+'rating.pickle')\n",
    "# #save_pickle(docs, filename+'docs.pickle')\n",
    "# save_pickle(docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Training Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For Testing data\n",
    "# csr_mat2 = build_matrix(test_docs)\n",
    "# print 'Testing CSR Formed'\n",
    "# mat2 = csr_idf(csr_mat2, copy=True)\n",
    "# print 'Testing IDF Formed'\n",
    "# test_docs_csr = csr_l2normalize(mat2, copy=True)\n",
    "# print 'Testing L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Testing Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/testing/'\n",
    "# save_pickle(test_userid, filename+'userid.pickle')\n",
    "# save_pickle(test_rating, filename+'rating.pickle')\n",
    "# #save_pickle(test_docs, filename+'docs.pickle')\n",
    "# save_pickle(test_docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Testing Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create CSR Matrix for Merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create Merge Data\n",
    "# merged_userid = userid + test_userid\n",
    "# merged_rating = rating + test_rating\n",
    "# merged_docs = docs + test_docs\n",
    "\n",
    "# # CSR For Merged data\n",
    "# csr_mat3 = build_matrix(merged_docs)\n",
    "# print 'Merged CSR Formed'\n",
    "# mat3 = csr_idf(csr_mat3, copy=True)\n",
    "# print 'Merged IDF Formed'\n",
    "# merged_docs_csr = csr_l2normalize(mat3, copy=True)\n",
    "# print 'Merged L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Merged Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/merged/'\n",
    "# save_pickle(merged_userid, filename+'userid.pickle')\n",
    "# save_pickle(merged_rating, filename+'rating.pickle')\n",
    "# #save_pickle(merged_docs, filename+'docs.pickle')\n",
    "# save_pickle(merged_docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Merged Data saved into pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
