{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading Test and training data set we created from yelp data set.\n",
    "This data is cleansed and converted to CSR Matrix. This CSR Matrix is huge hence storing it as pickle for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting full Data to CSR Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/reviews_full.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()  \n",
    "no_of_samples=50000\n",
    "lines = [lines[i] for i in range(0,no_of_samples)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "lines[0]\n",
    "print len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Number of lines processed: 50000\n",
      "Training Data: Length of userid array: 50000\n",
      "Training Data: Length of rating array: 50000\n",
      "Training Data: Length of docs array: 50000\n",
      "Training Data: Length of business array: 50000\n",
      "Training Data: Number of exceptions encountered: 0\n"
     ]
    }
   ],
   "source": [
    "userid = []\n",
    "rating = []\n",
    "docs = []\n",
    "business = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "error_line_num = []\n",
    "error_lines = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        i = i + 1\n",
    "        l = line.split('\\t', 4)\n",
    "        userid.append(l[0])\n",
    "        business.append(l[1])\n",
    "        rating.append(l[2])\n",
    "        #d = clean(l[3]).split()\n",
    "        #kmers = getKmers(d)\n",
    "        #d.extend(kmers)\n",
    "        docs.append(l[3])\n",
    "    except Exception as e:\n",
    "        j = j + 1\n",
    "        error_line_num.append(i)\n",
    "        error_lines.append(line)\n",
    "        print e\n",
    "\n",
    "print 'Training Data: Number of lines processed: ' + str(i)\n",
    "print 'Training Data: Length of userid array: ' + str(len(userid))\n",
    "print 'Training Data: Length of rating array: ' + str(len(rating))\n",
    "print 'Training Data: Length of docs array: ' + str(len(docs))\n",
    "print 'Training Data: Length of business array: ' + str(len(business))\n",
    "print 'Training Data: Number of exceptions encountered: ' + str(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSR Matrix for complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csr_mat = build_matrix(docs)\n",
    "print 'Training CSR Formed'\n",
    "mat1 = csr_idf(csr_mat, copy=True)\n",
    "print 'Training IDF Formed'\n",
    "docs_csr = csr_l2normalize(mat1, copy=True)\n",
    "print 'Training L2Norm Formed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_samples = 50000\n",
    "n_features = 15000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "\n",
    "data_samples = docs[:n_samples]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_nmf = nmf.transform(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "\n",
    "vocab = np.array(tfidf_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle(tfidf_nmf, 'pickle/nmf.pickle')\n",
    "save_pickle(vocab, 'pickle/vocab.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pickle/'\n",
    "save_pickle(userid, filename+'userid.pickle')\n",
    "save_pickle(rating, filename+'rating.pickle')\n",
    "save_pickle(business, filename+'business.pickle')\n",
    "save_pickle(docs_csr, filename+'docs_csr.pickle')\n",
    "print 'Training Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Code to be ignored as it is for previous version of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"data/test_final.dat\", \"r\") as fh:\n",
    "#     test_lines = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_userid = []\n",
    "# test_rating = []\n",
    "# test_docs = []\n",
    "# test_i = 0\n",
    "# test_j = 0\n",
    "# test_error_line_num = []\n",
    "# test_error_lines = []\n",
    "# for line in test_lines:\n",
    "#     try:\n",
    "#         test_i = test_i + 1\n",
    "#         l = line.split('\\t', 3)\n",
    "#         test_userid.append(l[0])\n",
    "#         test_rating.append(l[1])\n",
    "#         d = clean(l[2]).split()\n",
    "#         kmers = getKmers(d)\n",
    "#         d.extend(kmers)\n",
    "#         test_docs.append(d)\n",
    "#     except Exception as e:\n",
    "#         test_j = test_j + 1\n",
    "#         test_error_line_num.append(test_i)\n",
    "#         test_error_lines.append(line)\n",
    "#         print e\n",
    "\n",
    "# print 'Testing Data: Number of lines processed: ' + str(test_i)\n",
    "# print 'Testing Data: Length of userid array: ' + str(len(test_userid))\n",
    "# print 'Testing Data: Length of rating array: ' + str(len(test_rating))\n",
    "# print 'Testing Data: Length of docs array: ' + str(len(test_docs))\n",
    "# print 'Testing Data: Number of exceptions encountered: ' + str(test_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For Training data\n",
    "# csr_mat = build_matrix(docs)\n",
    "# print 'Training CSR Formed'\n",
    "# mat1 = csr_idf(csr_mat, copy=True)\n",
    "# print 'Training IDF Formed'\n",
    "# docs_csr = csr_l2normalize(mat1, copy=True)\n",
    "# print 'Training L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/training/'\n",
    "# save_pickle(userid, filename+'userid.pickle')\n",
    "# save_pickle(rating, filename+'rating.pickle')\n",
    "# #save_pickle(docs, filename+'docs.pickle')\n",
    "# save_pickle(docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Training Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For Testing data\n",
    "# csr_mat2 = build_matrix(test_docs)\n",
    "# print 'Testing CSR Formed'\n",
    "# mat2 = csr_idf(csr_mat2, copy=True)\n",
    "# print 'Testing IDF Formed'\n",
    "# test_docs_csr = csr_l2normalize(mat2, copy=True)\n",
    "# print 'Testing L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Testing Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/testing/'\n",
    "# save_pickle(test_userid, filename+'userid.pickle')\n",
    "# save_pickle(test_rating, filename+'rating.pickle')\n",
    "# #save_pickle(test_docs, filename+'docs.pickle')\n",
    "# save_pickle(test_docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Testing Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create CSR Matrix for Merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create Merge Data\n",
    "# merged_userid = userid + test_userid\n",
    "# merged_rating = rating + test_rating\n",
    "# merged_docs = docs + test_docs\n",
    "\n",
    "# # CSR For Merged data\n",
    "# csr_mat3 = build_matrix(merged_docs)\n",
    "# print 'Merged CSR Formed'\n",
    "# mat3 = csr_idf(csr_mat3, copy=True)\n",
    "# print 'Merged IDF Formed'\n",
    "# merged_docs_csr = csr_l2normalize(mat3, copy=True)\n",
    "# print 'Merged L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Merged Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'pickle/merged/'\n",
    "# save_pickle(merged_userid, filename+'userid.pickle')\n",
    "# save_pickle(merged_rating, filename+'rating.pickle')\n",
    "# #save_pickle(merged_docs, filename+'docs.pickle')\n",
    "# save_pickle(merged_docs_csr, filename+'docs_csr.pickle')\n",
    "# print 'Merged Data saved into pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
