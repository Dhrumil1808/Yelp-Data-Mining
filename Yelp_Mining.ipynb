{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Mining Project\n",
    "## CMPE - 255 Data Mining Fall 2017\n",
    "### Group 6\n",
    "- Dhrumil Shah\n",
    "- Nishant Rathi\n",
    "- Rashmi Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to Preprocess Data and create CSR\n",
    "\n",
    "In this Jupyter Notebook, we are loading Test and training data set we created from yelp data set.\n",
    "This data is cleansed and converted to CSR Matrix. This CSR Matrix is huge hence storing it as pickle for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import cPickle as pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This block have functions for:\n",
    "\n",
    "- clean - remove punctuations, small words and convert allwords to lower\n",
    "- kmers - form kmers by grouping 2 and 3 words\n",
    "- csr - form csr matrix, idf and l2 norm of csr matrix\n",
    "- pickle - to write to a pickle and read from a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, ' ', raw)\n",
    "  cleanr = re.compile('[^a-zA-Z0-9]')\n",
    "  cleantext = re.sub(cleanr, ' ', cleantext)\n",
    "  cleanr = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "  cleantext = re.sub(cleanr, '', cleantext)\n",
    "  return cleantext.lower()\n",
    "\n",
    "def group(inp, n = 2):\n",
    "    for i in xrange(len(inp) - (n - 1)):\n",
    "        yield inp[i:i+n]\n",
    "\n",
    "def group2words(inp):\n",
    "    comb_2_words = []\n",
    "    for f, s in group(inp, 2):\n",
    "        comb = f + \" \"+s\n",
    "        comb_2_words.append(comb)\n",
    "    return comb_2_words\n",
    "\n",
    "def group3words(inp):\n",
    "    comb_3_words = []\n",
    "    for f, s, t in group(inp, 3):\n",
    "        comb = f + \" \"+s + \" \"+t\n",
    "        comb_3_words.append(comb)\n",
    "    return comb_3_words\n",
    "\n",
    "def getKmers(inp):\n",
    "    kmers=[]\n",
    "    comb_2_words = group2words(inp)\n",
    "    for comb in comb_2_words:\n",
    "        kmers.append(comb)\n",
    "    comb_3_words = group3words(inp)\n",
    "    for comb in comb_3_words:\n",
    "        kmers.append(comb)\n",
    "    return kmers\n",
    "\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents,\n",
    "    each of which is a list of word/terms in the document.\n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    # Remove all ratings\n",
    "    for d in docs:\n",
    "        #d = d[1:]\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        #d = d[1:]\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "def save_pickle(matrix, filename):\n",
    "    with open(filename, 'wb') as outfile:\n",
    "        pickle.dump(matrix, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as infile:\n",
    "        matrix = pickle.load(infile)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_final.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Number of lines processed: 78790\n",
      "Training Data: Length of userid array: 78790\n",
      "Training Data: Length of rating array: 78790\n",
      "Training Data: Length of docs array: 78790\n",
      "Training Data: Number of exceptions encountered: 0\n"
     ]
    }
   ],
   "source": [
    "userid = []\n",
    "rating = []\n",
    "docs = []\n",
    "i = 0\n",
    "j = 0\n",
    "error_line_num = []\n",
    "error_lines = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        i = i + 1\n",
    "        l = line.split('\\t', 3)\n",
    "        userid.append(l[0])\n",
    "        rating.append(l[1])\n",
    "        d = clean(l[2]).split()\n",
    "        kmers = getKmers(d)\n",
    "        d.extend(kmers)\n",
    "        docs.append(d)\n",
    "    except Exception as e:\n",
    "        j = j + 1\n",
    "        error_line_num.append(i)\n",
    "        error_lines.append(line)\n",
    "        print e\n",
    "\n",
    "print 'Training Data: Number of lines processed: ' + str(i)\n",
    "print 'Training Data: Length of userid array: ' + str(len(userid))\n",
    "print 'Training Data: Length of rating array: ' + str(len(rating))\n",
    "print 'Training Data: Length of docs array: ' + str(len(docs))\n",
    "print 'Training Data: Number of exceptions encountered: ' + str(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/test_final.dat\", \"r\") as fh:\n",
    "    test_lines = fh.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data: Number of lines processed: 21210\n",
      "Testing Data: Length of userid array: 21210\n",
      "Testing Data: Length of rating array: 21210\n",
      "Testing Data: Length of docs array: 21210\n",
      "Testing Data: Number of exceptions encountered: 0\n"
     ]
    }
   ],
   "source": [
    "test_userid = []\n",
    "test_rating = []\n",
    "test_docs = []\n",
    "test_i = 0\n",
    "test_j = 0\n",
    "test_error_line_num = []\n",
    "test_error_lines = []\n",
    "for line in test_lines:\n",
    "    try:\n",
    "        test_i = test_i + 1\n",
    "        l = line.split('\\t', 3)\n",
    "        test_userid.append(l[0])\n",
    "        test_rating.append(l[1])\n",
    "        d = clean(l[2]).split()\n",
    "        kmers = getKmers(d)\n",
    "        d.extend(kmers)\n",
    "        test_docs.append(d)\n",
    "    except Exception as e:\n",
    "        test_j = test_j + 1\n",
    "        test_error_line_num.append(test_i)\n",
    "        test_error_lines.append(line)\n",
    "        print e\n",
    "\n",
    "print 'Testing Data: Number of lines processed: ' + str(test_i)\n",
    "print 'Testing Data: Length of userid array: ' + str(len(test_userid))\n",
    "print 'Testing Data: Length of rating array: ' + str(len(test_rating))\n",
    "print 'Testing Data: Length of docs array: ' + str(len(test_docs))\n",
    "print 'Testing Data: Number of exceptions encountered: ' + str(test_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CSR Formed\n",
      "Training IDF Formed\n",
      "Training L2Norm Formed\n"
     ]
    }
   ],
   "source": [
    "# For Training data\n",
    "csr_mat = build_matrix(docs)\n",
    "print 'Training CSR Formed'\n",
    "mat1 = csr_idf(csr_mat, copy=True)\n",
    "print 'Training IDF Formed'\n",
    "docs_csr = csr_l2normalize(mat1, copy=True)\n",
    "print 'Training L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data saved into pickle\n"
     ]
    }
   ],
   "source": [
    "filename = 'pickle/training/'\n",
    "save_pickle(userid, filename+'userid.pickle')\n",
    "save_pickle(rating, filename+'rating.pickle')\n",
    "#save_pickle(docs, filename+'docs.pickle')\n",
    "save_pickle(docs_csr, filename+'docs_csr.pickle')\n",
    "print 'Training Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSR Matrix for Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CSR Formed\n",
      "Testing IDF Formed\n",
      "Testing L2Norm Formed\n"
     ]
    }
   ],
   "source": [
    "# For Testing data\n",
    "csr_mat2 = build_matrix(test_docs)\n",
    "print 'Testing CSR Formed'\n",
    "mat2 = csr_idf(csr_mat2, copy=True)\n",
    "print 'Testing IDF Formed'\n",
    "test_docs_csr = csr_l2normalize(mat2, copy=True)\n",
    "print 'Testing L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Testing Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data saved into pickle\n"
     ]
    }
   ],
   "source": [
    "filename = 'pickle/testing/'\n",
    "save_pickle(test_userid, filename+'userid.pickle')\n",
    "save_pickle(test_rating, filename+'rating.pickle')\n",
    "#save_pickle(test_docs, filename+'docs.pickle')\n",
    "save_pickle(test_docs_csr, filename+'docs_csr.pickle')\n",
    "print 'Testing Data saved into pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create CSR Matrix for Merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSR Formed\n",
      "Merged IDF Formed\n",
      "Merged L2Norm Formed\n"
     ]
    }
   ],
   "source": [
    "# Create Merge Data\n",
    "merged_userid = userid + test_userid\n",
    "merged_rating = rating + test_rating\n",
    "merged_docs = docs + test_docs\n",
    "\n",
    "# CSR For Merged data\n",
    "csr_mat3 = build_matrix(merged_docs)\n",
    "print 'Merged CSR Formed'\n",
    "mat3 = csr_idf(csr_mat3, copy=True)\n",
    "print 'Merged IDF Formed'\n",
    "merged_docs_csr = csr_l2normalize(mat3, copy=True)\n",
    "print 'Merged L2Norm Formed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Merged Data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data saved into pickle\n"
     ]
    }
   ],
   "source": [
    "filename = 'pickle/merged/'\n",
    "save_pickle(merged_userid, filename+'userid.pickle')\n",
    "save_pickle(merged_rating, filename+'rating.pickle')\n",
    "#save_pickle(merged_docs, filename+'docs.pickle')\n",
    "save_pickle(merged_docs_csr, filename+'docs_csr.pickle')\n",
    "print 'Merged Data saved into pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
